{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kote\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\kote\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "C:\\Users\\kote\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4095</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>weâre   to reveal something for us proud sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16444</td>\n",
       "      <td>16445</td>\n",
       "      <td>0</td>\n",
       "      <td>what? that is stereotyping. and you know it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6540</td>\n",
       "      <td>6541</td>\n",
       "      <td>0</td>\n",
       "      <td>@user thanks. i will. just called mohammed on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21018</td>\n",
       "      <td>21019</td>\n",
       "      <td>0</td>\n",
       "      <td>@user that's both   and #hysterical! i'm kind ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19880</td>\n",
       "      <td>19881</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm moving to toronto in a few monthsð</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet\n",
       "4095    4096      0  weâre   to reveal something for us proud sou...\n",
       "16444  16445      0  what? that is stereotyping. and you know it is...\n",
       "6540    6541      0  @user thanks. i will. just called mohammed on ...\n",
       "21018  21019      0  @user that's both   and #hysterical! i'm kind ...\n",
       "19880  19881      0        i'm moving to toronto in a few monthsð  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('train_tweets.csv')\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\". Для этого создадим\n",
    "функцию:\n",
    "● для того, чтобы найти все вхождения паттерна в тексте, необходимо\n",
    "использовать re.findall(pattern, input_txt)\n",
    "● для для замены @user на пробел, необходимо использовать re.sub()\n",
    "\n",
    "2. Изменим регистр твитов на нижний с помощью .lower().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_dogs(input_txt):\n",
    "    pattern = \"@[\\w]*\"\n",
    "    item = re.findall(pattern, input_txt)\n",
    "    \n",
    "    try:\n",
    "        return re.sub(item[0],\"\", input_txt.lower())\n",
    "    except:\n",
    "        return input_txt.lower()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      when a father is dysfunctional and is so sel...\n",
       "1      thanks for #lyft credit i can't use cause th...\n",
       "2                                  bihday your majesty\n",
       "3    #model   i love u take with u all the time in ...\n",
       "4               factsguide: society now    #motivation\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"][:5].apply(lower_dogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Заменим сокращения с апострофами (пример: ain't, can't) на пробел, используя\n",
    "apostrophe_dict. Для этого необходимо сделать функцию: для каждого слова в\n",
    "тексте проверить (for word in text.split()), если слово есть в словаре apostrophe_dict в\n",
    "качестве ключа (сокращенного слова), то заменить ключ на значение (полную\n",
    "версию слова).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what be\n",
      "must 've\n",
      "tell\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "texts = [\"what's\", \"must've\", \"told\"]\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "for text in texts:\n",
    "        doc = nlp(text)\n",
    "        lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "        print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":) :(\n",
      "happy sad\n"
     ]
    }
   ],
   "source": [
    "def apostrophe(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    #emoji\n",
    "    phrase = re.sub(r\"\\:\\)\", \" happy\", phrase)\n",
    "    phrase = re.sub(r\"\\:\\(\", \" sad\", phrase)\n",
    "    \n",
    "    #Заменим пунктуацию на пробелы, используя re.sub() и паттерн r'[^\\w\\s]'.\n",
    "    phrase = re.sub(r'[^\\w\\s]', \" \", phrase)\n",
    "    \n",
    "    #Заменим спец. символы на пробелы, используя re.sub() и паттерн r'[^a-zA-Z0-9]'.\n",
    "    phrase = re.sub( r'[^a-zA-Z0-9]', \" \", phrase)\n",
    "    \n",
    "    #Заменим числа на пробелы, используя re.sub() и паттерн r'[^a-zA-Z]'.\n",
    "    phrase = re.sub( r'[^a-zA-Z]', \" \", phrase)\n",
    "    \n",
    "    #Удалим из текста слова длиной в 1 символ, используя ' '.join([w for w in x.split() if len(w)>1]).\n",
    "    phrase = ' '.join([w for w in phrase.split() if len(w)>1]) \n",
    "    return phrase\n",
    "\n",
    "test = \":) :(\" #data[\"tweet\"][1]\n",
    "print(test)\n",
    "print(apostrophe(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"token\"] = data[\"tweet\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  bihday your majesty'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"tweet\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим стоп-слова из токенов, используя nltk.corpus.stopwords. Создадим столбец 'tweet_token_filtered' без стоп-слов.\n",
    "Применим стемминг к токенам с помощью nltk.stem.PorterStemmer. Создадим столбец 'tweet_stemmed' после применения стемминга.\n",
    "Применим лемматизацию к токенам с помощью nltk.stem.wordnet.WordNetLemmatizer. Создадим столбец 'tweet_lemmatized' после применения лемматизации.\n",
    "Сохраним результат предобработки в pickle-файл.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_stop_words(words):\n",
    "   \n",
    "    return [word for word in words if not word in stop_words]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"token_filtered\"] = data[\"token\"].apply(del_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer_words(words):\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def lemmatizer_words(words):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"stemmer_token_filtered\"] = data[\"token_filtered\"].apply(stemmer_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lemmatizer_token_filtered\"] = data[\"token_filtered\"].apply(lemmatizer_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
